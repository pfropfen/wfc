
wenn die tiles zu einer bestimmten lösung tendieren, zb wenn einmal schneemann dann weniger möglichkeiten
vielleicht stuckt man am schneemann/bergschnee fest..

graphen von wahrscheinlichkeiten erstellen?

biombildung


vielleicht nicht IMMER die niedrigste entropie wählen


wie ist korrelation zwischen der anzahl an verbindungen die du hast zu der wahrscheinlichkeit auf der map zu erscheinen

MEHRERE STARTPUNKTE

GRAPH FÜR TILES


REDDIS für nachriuchten zwischen diensten


TSP vergleich / Adjazenzmatrix
debugging schwer, wie testet man die logik ausreichend?
vielleicht umlet file nutzen

-> probiert: alle 9 drum herum werden restricted
-> nicht immer niedrigste entropie sondern auch zweitniedrigste / und 3niedrichste
-> restriction für wald zurückändern
-> alle entropien nehmen aber mit wahrscheinlichkeiten versehen
-> zentrierte excel-tabelle mit restrictions
-> wavefunctionlookup ändern, exel-tabelle auslesen
-> entropyTolerance einbauen
-> höhere tolerance erhöht die performance
-> verteilung dynamisch in numberOfParts
-> ADJAZENZ MATRIX mit checkboxen
-> zentrierte restrictions

-> kafka aufsetzen
-> frontend für distributor
-> architektur überlegen 
-> mysql db für hub
-> kafka ersetzt durch rabbitmq
-> ssl probleme
-> 
192.168.1.33




--------------
N O T E S:

------------------------------------------------------------------
LIMITATIONS:
map immer quadratisch aus praktischen gründen






TODO:
-----

STRETCH GOAL: NAS SERVICE VOLUME SHIT
-> 1 node mit custom label, timedb nur darauf starten

RIESIGE PRINTS ENTFERNEN!!

EVALUTION: maps teilen in 2er oder 3er systemen.. zahlen wählen die möglichst niedrige potenz hat?
           was kann alles evaluiert werden?

k^2/p element von N (natürliche zahlen)
(k= kantenlänge, p= anzahl parts)
p muss teil der menge der kombination der primfaktorisierung des quadrates der zahl sein


WEBSEITEN FÜR DISTRIBUTOR+MANAGER VIELLEICHT ETWAS SCHÖNER

KAPITEL: SPEZIELLE HERAUSFORDERUNGEN / PROBLEME


NOTES:
------
lastChunkEndTime, mapStartTime, chunkStartTime,....  wenn komplexität steigt, werden sprechende bezeichnungen und so immer wichtiger
die prinzipien die man im studium gelernt hat werden tatsächlich irgendwann nützlich

für das dokument: verwendete libraries -> pandas, openpyxl, ...

DOCKERFILE geändert -> ADD . . nach RUN..  für deutlich bessere performance!

-> volume wird immer woanders gestartet, änderung auf StatefulSet 
jetzt immer auf vm2

GET DATA FROM TIMEDB -> CSV/EXCEL

EVENTBASED ALS AUSBLICK/LIMITATION -- SIND WIR VIELLEICHT SCHON EVENTBASED? WAS FEHLT NOCH
MICROSERVICES? ALS AUSBLICK

WIE SIEHTS MIT SKALIERUNG AUS? datenbanken?

---> NOTIZ: KAFKA PROBIERT DANN RABBITMQ


--> KAPITEL FÜR PROBLEME/LÖSUNGEN/ERKENNTNISSE/ LESSONS LERNEN irgendwie so was

--> WORTVERZEICHNIS: Map, Chunks, Tiles, EntropyTolerance, Worker, Rules, WFC


dran denken dass wave/wavelookup auch in worker und maploader vorhanden sind

--> für kubernetes kapitel: kubectl, kudeadm und kubecli



VM NOTES:


win: 192.168.1.190
vm:  192.168.1.92


vm1: 192.168.1.93
vm2: 192.168.1.94
vm3: 192.168.1.95

kubeadm join 192.168.1.93:6443 --token 6rqx8k.5vw127fxc5vw6m11 \
        --discovery-token-ca-cert-hash sha256:cd99502a3e26aa6cade22b28f07d716c3f699dfae012baae03641b82e04cca46


kubeadm join 192.168.1.93:6443 --token 6rqx8k.5vw127fxc5vw6m11 --discovery-token-ca-cert-hash sha256:cd99502a3e26aa6cade22b28f07d716c3f699dfae012baae03641b82e04cca46



kubectl get nodes      // nodes zeigen
kubectl get pods -A    // pods zeigen



auf master (in home/user/):
sudo kubeadm config images pull
kubeadm init
sudo kubeadm init --pod-network-cidr=10.10.0.0/16 > token.sh
[token.sh hochladen und editieren]

mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config


auf node (in home/user/):
[token.sh runterladen/speichern]
sudo bash token.sh

auf master:
kubectl create -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/tigera-operator.yaml
curl https://raw.githubusercontent.com/projectcalico/calico/v3.26.1/manifests/custom-resources.yaml -O
ls -l
sed -i 's/cidr: 192\.168\.0\.0\/16/cidr: 10.10.0.0\/16/g' custom-resources.yaml
kubectl create -f custom-resources.yaml




MANAGER:
sudo docker build -t imgmanager .
image: wfcmanager
sudo docker run -p 5000:5000 -d --name manager imgmanager


kubectl apply -f wfcdeploy.yaml --namespace=wfc
kubectl delete -f wfcdeploy.yaml

kubectl describe pod NAME
kubectl logs NAME

sudo service docker restart
sudo systemctl restart docker
sudo systemctl daemon-reload
sudo systemctl stop docker

docker run -d -p 9000:5000 --restart always --name registry registry:2

(WEIL MIT SNAP INSTALLIERT)
sudo nano /var/snap/docker/current/config/daemon.json   <- insecure registry eintragen
sudo snap stop docker

CONTAINERD insecure-registry hinzufügen:
sudo nano /etc/containerd/config.toml
sudo systemctl restart containerd


SECURE REGISTRY:
htpasswd -Bbc /root/auth/htpasswd root password
cat /root/auth/htpasswd

openssl ecparam -name prime256v1 -genkey -noout -out certs/server.key
openssl req -new -sha256 -key certs/server.key -out certs/server.csr

openssl req -newkey rsa:4096 -nodes -sha256 -keyout certs/server.key -x509 -days 365 -out certs/server.crt

docker run -d --rm -p 9000:443 \
-v /root/auth/:/opt/auth -v /root/certs/:/opt/certs \
-e REGISTRY_HTTP_ADDR=0.0.0.0:443 \
-e REGISTRY_HTTP_TLS_CERTIFICATE=/opt/certs/server.crt \
-e REGISTRY_HTTP_TLS_KEY=/opt/certs/server.key \
-e REFISTRY_AUTH=htpasswd \
-e REGISTRY_AUTH_HTPASSWD_REALM=Registry-Realm \
-e REGISTRY_AUTH_HTPASSWD_PATH=/opt/auth/htpasswd \
--name registry2 registry:9000/registry:2


curl https://192.168.1.93:9000/v2/_catalog
curl -Isk https://192.168.1.93:9000/v2

sudo docker login https://192.168.1.93:9000 -u root  (password)

(DANN IMAGES PUSHEN)


kubectl create secret docker-registry dockerauth --docker-username=root --docker-password=password


DOCKER HUB:
push images:
docker tag local-image:tagname new-repo:tagname
docker push new-repo:tagname

docker push pfropfen/imgworker:tagname


sudo docker login -u pfropfen
(passwort)
bash push.sh


kubectl delete --all pods --namespace=foo
kubectl delete --all deployments --namespace=foo
kubectl delete --all namespaces

kubectl delete --all pods --grace-period=0 --force
for p in $(kubectl get pods | grep Terminating | awk '{print $1}'); do kubectl delete pod $p --grace-period=0 --force;done



kubectl get pods -o wide
kubectl get pods [podname] -o yaml
sudo systemctl restart kubelet
kubectl delete pod --force


https://192.168.1.93:6443/api/v1/namespaces/default/services/http:distributor:5001/proxy/mapGenerator
http://192.168.1.93:31001/mapGenerator


kubectl create namespace wfc
kubectl apply -f wfcdeploy.yaml --namespace=wfc
kubectl get pods --namespace=wfc


kubectl delete -f wfcdeploy.yaml
